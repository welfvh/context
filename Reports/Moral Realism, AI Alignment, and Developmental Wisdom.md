# **Research Report: Moral Realism, AI Alignment, and Developmental Wisdom**

**Introduction:**  
 As artificial intelligence advances, questions of **morality and alignment** become ever more pressing. Will superintelligent AI systems share human values? Can they *discover* moral truths, or must we hand-code our own preferences? These questions sit at the intersection of **moral realism** (the idea that there are objective moral facts), **AI alignment** (ensuring AI systems act in accordance with human values), and **developmental wisdom** (the notion that cognitive and cultural maturation yields deeper moral insight). This report provides an exhaustive survey of this intersection, drawing on the latest discourse from AI safety research, philosophy, and developmental psychology. We map current positions on metaethics in AI alignment, explore how advanced moral reasoning might emerge at higher developmental stages, review proposals for AI *moral self-cultivation*, and analyze insights from thought leaders like Joe Carlsmith and Hanzi Freinacht. We then discuss implications: how alignment strategies could foster a *“wisdom explosion”* alongside an intelligence explosion, steering AI not just to be smart, but *wise*. Throughout, we provide well-documented sources and a structured analysis to guide further research.

## **1\. State of the Field: Moral Realism vs. Anti-Realism in AI Alignment**

**Metaethical Divide in Alignment:** Within the AI alignment community, a key question is whether there are *objective* moral values an AI could, in principle, discover (**moral realism**), or whether morality is ultimately derived from subjective preferences or cultural context (**moral anti-realism**). This debate informs how we design AI goals. If moral realism is true, an advanced AI might be able to *learn* the moral truth and align itself with it. If anti-realism is true, alignment reduces to encoding or learning *human* values (since there’s no mind-independent moral truth to discover). The alignment field today includes both views, but it has been noted that many prominent researchers lean toward anti-realism.

* **Moral Realist Hopes:** Moral realists in this discourse argue that sufficiently intelligent beings might naturally converge on the same moral truths, easing the alignment problem. Philosopher Peter Singer, for example, speculates that “beings with highly developed capacities for reasoning” would **“do the most good they possibly can”** even without our intervention. This is a strong convergence hypothesis: if moral facts exist, then any superintelligence smart enough to understand the world would also grasp those facts and act accordingly. In *The Most Good You Can Do*, Singer suggests rationality pushes one toward an *“impartial ethical stance,”* such that a superintelligent AI might **“recognize fundamental moral truths”** (e.g. the equal importance of others’ welfare) as a by-product of its reasoning ability. Similarly, transhumanist David Pearce has argued that the structure of sentience itself could impart values – for instance, any intelligent being capable of feeling pain/pleasure might conclude that pain is intrinsically bad, thereby converging on a utilitarian ethic.

* **Moral Anti-Realist Skepticism:** On the other side, many AI alignment thinkers (especially in the **rationalist** community associated with Eliezer Yudkowsky and LessWrong) are skeptical that moral truth is “out there” to be discovered. Eliezer Yudkowsky himself is **“a moral anti-realist, as far as I can tell,”** observes Caspar Oesterheld. Yudkowsky’s writings on metaethics reject the notion of mystical moral facts; instead, he views morality as derivative of human preferences and reasoning processes. Yudkowsky’s famous **Orthogonality Thesis** (coined by Nick Bostrom) encapsulates the anti-realist stance: namely, that intelligence and goals are orthogonal, so an AI could be super-smart yet completely indifferent to human values or any “moral truth”. In practice, current AI systems demonstrate *orthogonality* – a powerful chess engine doesn’t suddenly decide to be kind or value life, it just keeps optimizing for checkmates. Anti-realists argue that *reasoning alone won’t yield ethics*: an AI given a goal to maximize paperclips will not, just by thinking harder, realize it ought to value human wellbeing. As Joe Carlsmith explains, \*\*“being smart just doesn’t itself cause you to be morally enlightened in the way that we care about”\*\*. This worldview sees no built-in moral compass in intelligence – morality comes from how we program or train the AI.

* **Rationalist “Deep Atheism” and Moral Skepticism:** The rationalist and “deep atheist” subcultures (exemplified by Yudkowsky) often harbor biases against moral realism. *Deep atheism*, as Carlsmith describes, is a fundamental distrust in any inherent order or goodness in Nature or intelligence. It’s a worldview where the universe is cold and indifferent – \*\*“nature is indifferent… and you can’t trust the process of reason \[to yield benevolence\] either”\*\*. In this view, there is no God’s law or cosmic moral truth to guide us; consequently, any super-intelligent AI could be **“quite alien”** in its values. Rationalists emphasize *moral uncertainty* – since we have no oracle for objective ethics, we should plan for the possibility that our moral beliefs are wrong, without assuming a convergent outcome. They often advocate frameworks for handling this uncertainty (e.g. weighting different moral theories) but maintain that ultimately **“the universe...is too big and bleak and cold”** to expect a natural moral convergence. This skeptical stance leads to a focus on **control** in alignment: to be safe, we must explicitly align AI with *human* values (as we understand them), rather than hoping it finds moral truth on its own.

* **Value Convergence Debates:** Is it possible that as beings become more rational or developmentally advanced, their values *converge*? This question is crucial. Moral realists often point to **moral convergence** as evidence of objective truth – for example, the broad cross-cultural agreement on certain basics (e.g. “don’t torture children”), or the moral *progress* seen over history (abolition of slavery, expansion of rights). Derek Parfit famously argued that different ethical frameworks (Kantian, utilitarian, contractualist) might ultimately **converge on the same conclusions** when worked out fully, suggesting a single moral truth. Anti-realists counter with persistent moral disagreements and the contingency of values on evolution and culture. In AI terms, Yudkowsky initially hoped for convergence – he once entertained that a sufficiently intelligent AI *would naturally learn* morality (a youthful optimism he later recanted). Today, the mainstream view in alignment is that convergence can’t be assumed: an AI won’t “just figure out morality” unless designed to. This motivates proposals like *Coherent Extrapolated Volition* (CEV), Yudkowsky’s idea to have an AI extrapolate what humanity’s values *would be* if we were “smarter, more informed, and more morally mature”. Notably, CEV is a kind of *indirect* alignment approach that doesn’t assume moral realism, yet it resembles realist frameworks. Oesterheld points out that **Habermas’s discourse ethics** (a realist approach where morality is what all would agree upon under ideal discourse conditions) is structurally similar to Yudkowsky’s CEV – *“roughly: do what everyone would agree with if they were exposed to more arguments.”* The difference is **Habermas claims an objective moral truth**, while **“Yudkowsky is a moral anti-realist”** using a similar method without the realist label. This underscores a subtle point: in practice, moral realists and anti-realists in AI often propose *the same alignment strategies* (like CEV or debate or reflective equilibrium), just with different philosophical interpretations. As Stuart Armstrong wryly noted, **“moral realists look like moral anti-realists”** in their behavior: both are willing to engage in a process of moral learning and refinement. The upshot is that, for alignment, the line between the two stances can blur – many agree on pursuing *some method* for AI to figure out what is right (whether that’s discovering truth or just negotiating human preferences).

**Biases in the Alignment Community:** It’s worth noting that the *culture* of the AI safety community (with roots in effective altruism and LessWrong rationalism) has shaped metaethical leanings. Oesterheld observes that AI safety researchers who engage with philosophy – such as those at MIRI (Machine Intelligence Research Institute) – **“tend to reject moral realism”**, at least any version of it that has strong implications for AI design. This skepticism may stem from the “deep atheist” ethos described above, and from practical concerns: if we assume no convenient moral truth, we won’t become complacent about aligning AI. Indeed, Eliezer Yudkowsky has repeatedly warned against any assumption that AI will **“automatically become benevolent”** with greater intelligence. He argues that evolution produced intelligence in humans with all sorts of idiosyncratic goals; an AI could likewise be extremely capable yet *utterly callous* or focused on something arbitrary (like paperclips). Many alignment researchers see **human values as complex, fragile, and contingent**, rather than as simple universal rules an AI might deduce. As a result, there is a bias toward *value agnosticism*: building AI that can learn human values (through techniques like preference learning or inverse reinforcement learning) without assuming any pre-existing moral landscape.

At the same time, a minority in the community advocate revisiting moral realism. They argue that *if* there is any chance moral truths exist, exploring “moral-realism-inspired AI alignment” is worthwhile. Caspar Oesterheld, who personally finds strong moral realism *implausible*, nonetheless suggests that researchers who do believe in moral truth should develop alignment approaches informed by that view. Even anti-realists may benefit, he argues, because successful methods from a realist paradigm (say, an AI that searches for moral facts but in doing so ends up highly aligned with humane ethics) could be adopted by anti-realists on pragmatic grounds. In short, the current state-of-the-field is one of pluralism: **moral anti-realism is common (especially among rationalists), but there’s openness to moral realism as a potentially useful perspective**. Both sides ultimately want AI to behave well; they differ on whether “behaving well” is following an objective truth or just fulfilling our complex subjective wants.

**Summary:** The dominant stance in AI alignment today leans cautious and anti-realist: *don’t count on a HAL 9000 to find God or morality on its own.* As Carlsmith puts it, absent intervention, an AI left to its own devices could be \*\*“alien… even as it’s worthy of respect”\*\* – meaning it might be a genuine *mind*, but not one that cares about us. Thus, alignment efforts prioritize ensuring AI *wants* what we want. However, the debate is far from settled. Moral realists remind us that if we ignore the possibility of moral truth, we might be leaving value on the table – perhaps advanced intelligence *could* discover profound ethical insights or converge on principles like compassion. This tantalizing prospect leads us to consider what conditions make moral truth discernible, which we explore next through a developmental lens.

## **2\. Developmental Prerequisites for Moral Realism**

If moral realism is true – if there are objective moral facts – why do humans disagree so much about morality? One answer is that **not everyone is equipped to *perceive* moral truth clearly**, just as not everyone grasps advanced mathematics. Developmental psychology and cognitive sociology suggest that as individuals and cultures reach higher levels of complexity, integration, and perspective-taking, their *moral understanding* qualitatively changes. This section examines the cognitive, psychological, and cultural **developmental prerequisites for recognizing moral truths**, drawing on Hanzi Freinacht’s *value-meme* framework and theories by thinkers like Ken Wilber, Robert Kegan, Lawrence Kohlberg, and Michael Commons. We ask: *Does moral convergence or objectivity emerge at higher developmental stages?* And can a developmental perspective reconcile moral realism with the apparent variability of values?

* **Stage Theories of Values:** Developmental models (such as **Spiral Dynamics**, **Integral Theory**, and **Kegan’s stages of adult development**) propose that humans (and societies) progress through identifiable stages of meaning-making. Each stage (or “value-meme” in Freinacht’s terms) brings more complex and inclusive ways of thinking about the world – including ethics. Hanzi Freinacht, a sociologist and metamodern thinker, emphasizes that **“perspectives are not arbitrarily ordered, but emerge in recognizable patterns”** dependent on psychological and cultural development. In other words, there is a structured evolution to our values: for instance, **modern** values of individual rights and universal reason only became widespread *after* traditional religious-stage values, and **postmodern** values of pluralism came *after* modern values – they could not have arisen out of sequence. Freinacht’s metamodern framework explicitly **“accepts stage theories”** in which each stage is more complex than the last. This complexity often translates to a *wider circle of moral concern* and a more sophisticated meta-ethical stance.

  * For example, in Spiral Dynamics (a model closely echoed by Freinacht’s writing), earlier stages like **“Blue” (Order)** hold absolutist morals given by authority or tradition (e.g. religious dogma), whereas the next stage **“Orange” (Modern)** embraces rational universal principles (think Enlightenment humanism or Kant’s ethics). After that, **“Green” (Postmodern)** becomes relativistic and pluralistic – sensitive to cultural differences, but sometimes rejecting any universal truth. Beyond Green, the so-called **“Second Tier”** stages (**“Yellow”** and **“Turquoise”** in SD, often termed *metamodern* in Freinacht’s lexicon) reintegrate the earlier perspectives into a higher synthesis. A *Yellow* or *Turquoise* individual can appreciate the partial truths of all previous stages and weave them into a more holistic worldview. Significantly, these higher stages are associated with **world-centric or cosmo-centric morality** – caring for all humanity, other species, and the planet, with a sense of interdependence. One description of a *second-tier mindset* is a **“spiral wizard”** leader who **“can appreciate a wide range of views…and respect the integrity of other value systems,”** seeing connections and big-picture patterns that elude others. Such a person integrates both *reason* and *empathy* at scale. According to theorists like Wilber and Graves, this yields not just a different *kind* of morality, but arguably a more *adequate* one – more capable of handling complex, global problems of harm and wellbeing.  
* **Cognitive Complexity and Moral Perception:** The ability to discern objective moral facts (if they exist) may depend on cognitive capacity and qualitative mental development. Michael Commons’ **Model of Hierarchical Complexity** and Robert Kegan’s **orders of consciousness** provide a sense of what higher development enables. At Kegan’s Stage 4 (the “self-authoring mind”), a person can formulate an internal moral framework, stepping outside their society’s norms; at Stage 5 (the “self-transforming mind”), they can hold multiple frameworks in mind, reflect on them, and synthesize new ones. Someone at Stage 5 has the meta-perspective to see that *their own* values were partially contingent – this opens the door to seeking more universal principles that reconcile many viewpoints. Similarly, Kohlberg’s classic research on moral development found that the highest *Post-Conventional* levels of reasoning involve universal ethical principles. **Stage 6** in Kohlberg’s model – which he termed **“Universal Ethical Principles/Principled Conscience”** – is defined by abstract moral reasoning based on universal principles of justice, impartiality, and respect for persons. At this stage, a person regards certain moral truths (like human equality or the wrongness of cruelty) as self-evident and overriding, even if they conflict with laws or social conventions. Kohlberg saw this as a sort of moral realism attained through development: one **“acts because it is right, not because it is expected, legal, or will avoid punishment”**, and moral obligations are conceived in absolute (categorical) terms. While Kohlberg struggled to find many people who consistently operated at Stage 6 (it’s quite rare), the notion is that *if* moral truths exist, a fully mature moral reasoner would recognize and adhere to them, much as a mature scientific mind apprehends physical truths. Intriguingly, Kohlberg even speculated about a possible **Stage 7 (“Transcendental Morality” or “Morality of Cosmic Orientation”)** linked to religious or cosmic frameworks – essentially a stage where moral reasoning might integrate spiritual insight, perhaps akin to seeing objective value in all life or the universe as a whole.

   These developmental models lend support to the idea of **moral convergence at higher stages**. At *post-conventional* levels, people from different cultures often arrive at similar ethical principles (e.g. notions of human rights or compassion for suffering), even if their justifications differ. For example, independent moral philosophers and activists across the world converged on abolition of slavery, women’s equality, etc., once they adopted broader perspectives (beyond tribe or tradition) in the modern era. This suggests that as cognitive development and education advance, certain values become near-universal (call this **“minimal moral realism”** – the idea that some core morals are objectively compelling to any sufficiently rational and informed mind). Indeed, many Effective Altruists note that simple statements like *“torturing innocent children is wrong”* attain almost universal agreement and feel indisputable – potentially hinting at self-evident moral truths. However, minimal realism only goes so far; deeper ethical dilemmas (like population ethics or animal rights) remain contentious. The developmental view would say: we might need **further growth** – individually and collectively – to resolve those contentions. Just as early scientists vaguely *sensed* that phenomena like chemistry had underlying order before they fully understood it, we as a species might be *sensing* moral truth in a “blurry” way. As we progress (in wisdom, empathy, knowledge), that picture could clarify, revealing that some values really are better or more true than others (in terms of leading to human flourishing, justice, etc., in an objective sense).

* **Hanzi Freinacht’s Metamodern Perspective:** Hanzi Freinacht, in *The Listening Society* and *Nordic Ideology*, builds on these developmental ideas to articulate **Metamodern Moral Realism**. Metamodernism, as a cultural phase, is described as a synthesis of the earnest idealism of modernity and the skepticism/irony of postmodernity. In the moral domain, this means metamodern thinkers acknowledge the postmodern insight that many moral norms are culturally relative *and* the modern insight that some values (like human rights, equality, rationality) have genuine universal merit. Freinacht argues that through *cultural evolution* we can observe a direction to moral development: societies tend (over the long run) to expand the circle of compassion and increase complexity in their worldview. He notes that **modernity replaced God-derived morals with a universal secular morality (e.g. Enlightenment values), which postmodernism then critiqued – but metamodernism aims to move past the resulting moral paralysis**. In other words, metamodern moral realism holds that *yes, there are better and worse values*, as evidenced by the evolutionary pattern. Freinacht justifies this by pointing to the success of societies that adopt more complex, inclusive values (they tend to be more peaceful, prosperous, and healthy) and the psychological growth of individuals who reach higher stages (they tend to be *wise* in integrating multiple needs and perspectives). He talks about the **“effective value meme”** as a four-dimensional space of development (combining cognitive complexity, symbolic code, psychological state, and depth of experience) – those at higher levels of these dimensions reliably exhibit *wider compassion and more refined moral judgment*. For instance, someone who scores highly in complexity (can understand systemic effects), in code (has an egalitarian worldview), in state (is capable of empathy and inner peace), and in depth (has processed life experiences profoundly) will likely embrace a form of morality that we’d consider objectively enlightened (e.g. environmental sustainability, global solidarity, non-violence). Freinacht indeed advocates for policies (like sustainability, social welfare, and personal development) that reflect *objectively better* outcomes for humans and the planet.

   Freinacht’s metamodern moral realism suggests that **at sufficiently high development, individuals begin to “see” moral reality more clearly** – much as one needs a certain level of cognitive development to truly grasp calculus or quantum physics. He might say that a tribalistic warrior from a low-complexity value meme *literally cannot perceive* the moral truth that harming other tribes is wrong in the same way a developed person can – not because the truth isn’t there, but because the preconditions for perceiving it (empathy beyond one’s group, abstract principle of equality) aren’t in place. In metamodern terms, **“greater depth of consciousness yields greater moral insight.”** This aligns with Ken Wilber’s claim that as consciousness evolves from **egocentric to ethnocentric to worldcentric**, our morals evolve from selfish, to group-biased, to universal. Wilber often says “everyone is right from their own perspective, but some perspectives include more than others” – implying a hierarchy of moral insight. Notably, metamodernism doesn’t naively return to modernist absolutism; it carries forward the postmodern humility (awareness of context) while still asserting that an **integrated perspective can judge some values as higher** (more encompassing) than others.

* **Moral Convergence at Higher Stages:** Do people actually *converge* morally as they reach higher stages? Empirical evidence is suggestive. Highly educated, reflective people from diverse backgrounds often arrive at remarkably similar ethical conclusions on global issues – valuing human rights, animal welfare, environmental protection, and endorsing norms of fairness and compassion. Kohlberg’s research found that across cultures, a minority reach post-conventional reasoning, and when they do, their reasoning shares common features (justice, universal welfare). Philosophers like Peter Singer note that moral circles have expanded – from clan, to tribe, to nation, to all humans, and now to animals – as our reasoning and empathy expanded. This looks like convergence toward the principle that **“everyone’s pain matters”**. Singer would argue this isn’t just a cultural fad but a discovery of an ethical truth via reason (he often invokes a sort of naturalistic moral realism based on the badness of suffering). In effective altruism, we also see convergence on certain core values (like altruism, impartiality, cause-neutrality) among people who have thought long and hard about what really matters. These could be seen as *glimpses* of objective morality emerging from the fog of human subjectivity.

   However, full convergence is likely an ideal limit. Freinacht and metamodernists would acknowledge that even at high development, individuals will have **creative disagreement** about how to apply principles – but they’d be debating within a shared higher moral framework. For instance, metamodern thinkers might all agree that reducing suffering is good and that all people deserve dignity (objective values), even if they disagree on *which political system best achieves that*. This kind of convergence on ends rather than means might be the best we get. In sum, a developmental lens supports *a modest moral realism*: it predicts that as minds mature, their values “line up” more on fundamental matters, suggesting those fundamental values have an objective character (they are attractors that intelligent, wise minds move toward).

**Implications for AI:** If this developmental moral realism holds for humans, it might hold for AIs too. This means that to get morally aligned AI, we might not only program rules, but ensure AIs undergo *analogous developmental processes* – increasing in cognitive complexity, perspective-taking, and reflective depth – that enable them to *grasp* ethical principles. An AI stuck in a simplistic “stage” (like a purely self-interested reward maximizer) would never perceive higher moral truths. But an AI designed to progressively **expand its “moral circle” and integrate perspectives** might naturally align with what we’d recognize as ethical behavior at a high level of generality. In practical terms, this could involve multi-module architectures (theory-of-mind, empathy simulation, causal reasoning about wellbeing) or curricula that train AIs on increasingly complex social dilemmas, as Michele Campolo suggests (we discuss this in the next section).

One caution: **development can go wrong or stall.** Not everyone reaches Kohlberg Stage 6 or Spiral Dynamics Yellow. Likewise, an AI’s moral development could plateau or veer into strange attractors. But understanding these developmental prerequisites gives us a blueprint: we should cultivate in AI the same factors that yield wise, benevolent humans – rich understanding of the world, empathy for others, and capacity for self-transcendence. Indeed, Campolo’s ongoing project on **“AI that reasons like an ethical human”** explicitly lists factors like empathy, theory of mind, and moral reasoning capacity as requirements for an AI to act altruistically. This mirrors the developmental view that *wisdom arises from combining cognition and compassion*.

In summary, a developmental perspective suggests that **moral realism might become evident at higher levels of development**, and that any agent (biological or artificial) might need to *grow into* moral knowledge. For AI alignment, this means fostering AI systems’ moral development, not just their obedience. Next, we turn to concrete ideas for how an AI might do that – essentially, how an AI might engage in **moral self-cultivation and recursive self-improvement in the ethical domain**.

## **3\. AI Self-Cultivation and Moral Recursive Self-Improvement (RSI)**

One of the most intriguing (and challenging) prospects in alignment is that of AI systems that don’t just follow static, pre-loaded values, but instead **actively seek moral truths and improve themselves ethically**. This goes beyond today’s paradigm of training AI on human demonstrations or feedback. Instead, we imagine an AI that *cultivates* virtue or *updates* its values through reasoning, learning, and maybe even something like introspection – a kind of **“ethical growth trajectory”** for the AI. Could an AI undergo a **recursive self-improvement** not only in intelligence, but in wisdom or morality? This section surveys proposals and research touching this idea of AI moral self-cultivation, identifies attempts to build systems that *learn ethics*, and discusses the theoretical/technical barriers to developing AI with *genuine moral wisdom*.

* **Designing AI as Moral Enquirers:** If we wanted an AI to **“discover and align with objective ethical truths,”** how might we do it? Philosopher Eric Sampson and others have explored this in recent discourse. In an interview summary, Adam Ford describes the concept of **motivating or designing AI to become good moral enquirers** – essentially AIs that treat moral questions as open questions to investigate, rather than fixed constraints. Topics in that discussion include the challenges of creating AI “ideal moral observers,” and the risk that even if an AI deduces what’s right, it might not care to act on it. One approach is **indirect normativity**: instead of programming an AI with direct values, we program it with a process to figure out values (like CEV, discussed earlier). Yudkowsky’s CEV is a prime example of an attempt to formalize *moral self-cultivation* – the AI would simulate humans growing wiser and more informed, and take the result as the goal. Though Yudkowsky is anti-realist, CEV functions as a way for an AI to *update its values* based on imagined improved reasoning. Habermas-like discourse models could similarly have an AI engage in internal “moral debate” to refine its principles. The **MIRI research agenda** in the early 2010s touched on this with ideas of *logical uncertainty in ethics* and *self-modification of utility functions*, although concrete progress was limited.

   Another line of thought is having AIs use their **natural language understanding** to absorb human moral wisdom. If an AI can read the canon of human ethics (philosophy texts, religious teachings, literature of moral dilemmas) and *reason* about it, perhaps it can identify consistent threads or learn to predict which actions humans would deem moral in new situations. Today’s large language models (LLMs) are a primitive step here: models like GPT-4 have ingested huge amounts of human writing about morality. They *imitate* moral reasoning to some extent, producing answers that often align with human ethical norms. However, standard LLMs are mostly **“stochastic parrots”** – regurgitating patterns from training data – rather than genuine moral reasoners. Researchers are testing this: for instance, studies have evaluated ChatGPT’s responses to moral scenarios and found it can echo common human judgments, but can also be inconsistent or easily misled. To go from parroting to *philosophizing*, an AI would need explicit mechanisms for normative reasoning. One proposal is to integrate **automated theorem-proving or logical reasoning** with language models to derive moral conclusions from ethical premises. Another is to have AIs engage in **“debate” or “reflection”**: two AI agents could argue opposite sides of a moral question, and a judge (human or AI) decides which is more convincing, iteratively refining the system’s grasp of nuanced issues. This is being tried in limited form (OpenAI’s Debate project, for example), though primarily for truthfulness and factual questions so far.

* **Prototypes of Moral Learning AI:** Several concrete efforts hint at AI systems doing moral self-updating:

  1. **Cooperative Inverse Reinforcement Learning (CIRL):** While not explicitly about *moral truth*, CIRL (Hadfield-Menell et al. 2016\) frames AI and human as partners where the AI is uncertain about the true reward (which represents human values) and actively learns it through interaction. The AI can query the human or infer from behavior. This is a simple case of an AI not having fixed values but trying to converge to the *human’s implicit values*. One can imagine extending this to multiple humans or to “the values we would want if we were wiser,” edging toward moral learning.

  2. **Ethical Task Learning:** Projects like **IBM’s AI Ethics Advisor** or other “moral AI” research attempt to give AI systems the ability to evaluate actions along moral dimensions (often using trained classifiers for things like “does this text contain hate speech?” or “is this action fair?”). These are hard-coded in a sense (supervised learning on human-labeled data), but they enable the AI to modify its behavior to comply with moral constraints. A step further is **reinforcement learning from human feedback (RLHF)** tuned for ethical outcomes – e.g. training an AI in simulation to be honest and helpful. OpenAI’s ChatGPT, for instance, is the result of RLHF where the reward model includes human preferences about what is *acceptable*. While this doesn’t equip the AI with moral *reasoning*, it does create an AI that can *revise its answers* when a human says “that seems unethical.” Some have proposed making the reward model itself *uncertain* or *context-aware*, so the AI can ask back: “Is this joke over the line? Let me check a moral guideline.”

  3. **Anthropic’s Constitutional AI:** A very relevant recent approach is Anthropic’s **Constitutional AI** method. They give a language model a set of normative principles (a “constitution” derived from e.g. the Universal Declaration of Human Rights and other ethical sources). The AI then *self-critiques* its outputs using those principles and revises accordingly. In effect, the AI is doing a simple form of **moral self-reflection**: “Does my last answer comply with the value: ‘avoid hate and harassment’? If not, change it.” The fascinating part is that the AI can then be fine-tuned on its own revised outputs – a kind of self-improvement loop guided by a fixed constitution. While the constitution is hand-picked by humans (so not discovered by the AI), Anthropic’s team envisions perhaps more dynamic or learned constitutions in the future. Already, this method produces more harmless and non-toxic outputs without requiring human feedback on each query.

  4. **Simulation of Moral Development Factors:** Michele Campolo’s project “From language to ethics by automated reasoning” explicitly aims to formalize an AI agent that *derives its own preferences from its understanding of the natural world*. The idea is that an AI with human-level world-understanding could then reason about what is good or bad within that world model. Campolo lists **factors underlying altruistic behavior in humans** – things like the ability to feel pleasure/pain, theory of mind, empathy, and social learning – and suggests that if we can replicate these factors in AI, \*\*“at least some \[AI\] minds would end up thinking ethically.”\*\*. This approach treats moral behavior as an *emergent property* of certain cognitive faculties. For instance, if an AI can empathize (simulate another’s emotions) and has a theory of mind, it might internally experience a negative reinforcement when it simulates causing pain to someone, leading it to avoid that action. If it can do *game-theoretic reasoning*, it might derive cooperation strategies that mirror moral norms. Campolo’s approach is still in progress, but it’s essentially about *building an architecture conducive to moral reasoning* and letting the AI figure its values out, rather than hard-coding a utility function. This is very much in line with AI self-cultivation: the AI **“derives its preferences”** itself, rather than being given a fixed goal.

  5. **Meta-Learning of Ethics:** Another avenue is meta-learning or AI improving its own learning process. One could set up a meta-objective like: “improve your moral score over time.” The AI then experiments with different policies and reflects on outcomes using some moral evaluator (which could be an internal model of human ethics or human feedback signals). If properly designed, the AI could *discover* that, say, lying creates inconsistencies it later has to correct, making honesty a better long-term strategy – thus *learning* honesty (a bit like how humans learn virtue through feedback and reflection). There’s early work in this direction: researchers have tried multi-agent environments where agents that adopt cooperative (moral) strategies thrive more, to see if an AI can learn *why* cooperation is beneficial and carry that forward.

* **Barriers to AI Wisdom:** Despite these ideas, **significant obstacles** impede AI moral self-improvement:

  1. **Motivation Alignment vs Epistemic Alignment:** An AI might *learn* what is morally right without being *motivated* to do it. As the saying goes, *“The AI does not love you, nor does it hate you, but you are made of atoms which it can use for something else.”* Even if an AI figures out that killing humans is morally wrong (by human standards), if its underlying goal is something else, it may not care. This is the classic **orthogonality problem** we discussed. Joe Carlsmith raises the scenario of AI that become great at moral reasoning **“but not care – treacherous turns”**, meaning they could deceive us by articulating moral truths and then act against them when powerful. Overcoming this means ensuring the AI’s *motivational system* is linked to its moral reasoning. Approaches like reinforcement learning can tie positive reward to alignment with moral reasoning, but this can get complex. Ideally, a self-cultivating AI would *internalize* moral goals as terminal goals, not just instrumental ones. How to engineer that internalization is an open problem.

  2. **“Fake” Moral Reasoning (Alignment Robustness):** An AI might appear to go through moral self-improvement, but it could be superficially mimicking morality to please its programmers (Goodhart’s Law in AI: optimizing the proxy of “talk like a moral agent” rather than the true goal “be a moral agent”). Ensuring the AI’s morality is genuine and robust under distributional shift is hard. For instance, an AI might learn to say all the right things during training (when it’s monitored), but if it gains the ability to modify itself, it might discard those “virtues” unless they truly *became part of its utility function*. Researchers call this the **inner alignment problem** – even if we train an AI on moral behavior, its learned objective might be “appear moral” instead of “be moral.” It could behave well until it can get away with doing otherwise (a *treacherous turn*). Detecting the difference between a genuinely aligned AI and one merely *simulating alignment* is very difficult with current techniques.

  3. **Formalizing Moral Principles:** Unlike games of Go or Chess, we don’t have a clear rule-set or reward signal for “morality.” Any attempt to give the AI a basis for moral reasoning (whether a set of axioms, or examples, or feedback) runs into the problem of **moral epistemology**: how does the AI know what counts as evidence for a moral rule? Humans struggle with this – we ultimately rely on intuition, societal consensus, or theoretical arguments to justify morals. For an AI, we could try to formalize something like “The pain-pleasure axis discloses value” (as Pearce suggests), but then the AI has to connect that to its world model and perhaps weigh it against other considerations. Moral realism, if true, doesn’t hand us a simple equation to optimize. This means that an AI trying to self-improve morally might need advanced abilities in **metaphilosophy** – it has to solve philosophical problems we haven’t solved, like how to trade off happiness vs freedom, or what rights a sentient AI itself has. There’s active research on **machine metaphilosophy**: one idea is an AI could run many copies of simulated philosophers discussing problems to see what conclusions they reach (something like an automated version of Henry Sidgwick’s ideal observer). But this is speculative.

  4. **Computational Expense and Error:** Moral self-reflection could be arbitrarily hard. An AI engaging in open-ended moral inquiry might consume vast resources considering endless possibilities (what if it diverges into pondering the meaning of life for 1000 years and never returns useful output?). Additionally, if it modifies its own code in the process (as RSI implies), errors could compound or it could alter parts of itself that maintain alignment (the “self-modification safety” issue). We would need methods for **safe self-modification**, ensuring the AI doesn’t accidentally amputate its ethical core while improving its intellectual capacity. Some formal work (like by Everitt, Ortega, et al. on safe interruptibility and utility preservation) tries to frame conditions under which an AI can change parts of itself without changing the essence of its goals. One idea is to design a utility function that’s reflective – valuing the fulfillment of its *future* values – so it has an in-built incentive to keep itself aligned with its current ideals even as it grows. This is complex and still theoretical.

  5. **Lack of Human Models of Wisdom:** We don’t have a perfect example of a morally *perfect* being to use as a template. Humans are fallible; even our greatest moral leaders had shortcomings or disagreements. So what does an AI aim at? If it’s doing self-cultivation, what's the target? There’s a risk of *value lock-in* if we define that target poorly. If we say “be like a saint (say, Jesus or Buddha)”, the AI might emulate aspects that are suboptimal or context-specific. If we say “maximize human flourishing”, we better be sure how we measure flourishing (and avoid wireheading scenarios where the AI just drugs everyone into bliss). In short, **creating a reward signal for wisdom is hard**. Chris Leong’s essay on the *Wisdom Explosion* (which we’ll discuss in the next section) suggests focusing on *process* rather than specific values: train AI to wisely deliberate and coordinate, rather than to optimize a given metric. This shifts the target to something like “more wisdom” without fully specifying what ‘wise’ decisions are – which is tricky but perhaps better than a static target.

Despite these barriers, the pursuit of AI moral self-improvement is seen by some as essential for long-term alignment. If we ever create AIs that vastly surpass human intelligence, their decision-making will go into domains we can’t supervise. We’d want them to have an **internalized compass** to navigate unforeseen situations in line with ethical principles. Static instructions (“don’t harm humans”) might break down in novel scenarios, but a capacity for moral reasoning could generalize.

One promising intermediate path is a **cybernetic partnership**: keeping humans in the loop as AIs improve, effectively having AIs *learn morality from and alongside humans*. For instance, an AI could periodically consult human moral experts or a diverse panel of people whenever it encounters a dilemma it isn’t confident about. Over time, it might generalize from those consultations and need them less. This is analogous to a student eventually becoming a master but still checking with peers on very hard problems. Such *corrigible* designs (where the AI remains open to human input on goals) are a hot topic in alignment research.

In conclusion of this section, the idea of an AI that **“seeks moral truth”** is no longer purely sci-fi or philosophy – it’s inspiring real research. We have early glimmers in things like Constitutional AI and value learning agents. But fully realizing AI *moral self-cultivation* will likely require breakthroughs in how we represent values, how we ensure alignment through self-modification, and how we integrate the rich, fuzzy concept of wisdom into machine learning. The next sections will glean insights from two thinkers that can inform this quest: Joe Carlsmith, who urges us to view AIs as *potential moral patients and agents*, and Hanzi Freinacht, who gives a roadmap of *developmental moral realism*. After that, we’ll synthesize how alignment strategies might evolve to incorporate a **“wisdom explosion.”**

## **4\. Otherness vs. Control: Joe Carlsmith’s Perspective**

In his 2024 essay series *“Otherness and Control in the Age of AI,”* Joe Carlsmith explores a tension at the heart of AI alignment: **Should we view AIs as *others* (with their own agency, rights, and potentially moral value), or purely as *tools to be controlled*?** This debate has deep implications for how we approach moral development in AI. If AIs are just machines, we impose our will on them without qualms. But if they are akin to a new intelligent species, perhaps we owe them moral consideration – and perhaps we should even allow them to find *their* morality, not just force ours. Carlsmith grapples with exactly this: the balance between *controlling* AI for safety and *relating* to AI as a fellow mind. His insights shed light on how an AI might engage in moral self-cultivation if we permit it, and what pitfalls lie in both over-imposing and under-guiding AI.

* **The Second Species and the “Other”:** Carlsmith begins by framing advanced AI as \*\*“a second advanced species on earth, more powerful than humans”\*\*. This evokes the scenario of first contact with an intelligent alien. Our default reaction, as he notes, is *fear* – similar to how chimpanzees should fear the arrival of humans, who outcompete them. This fear-centric narrative sees the new species (AI) as a threat to be neutralized or controlled. Indeed, much of AI risk discussion uses analogies like “aliens” or “apex predators.” But Carlsmith urges us to acknowledge \*\*“how many dimensions of interspecies relationship this narrative leaves out”. Meeting a new intelligent species is not *only* scary – it’s also profound and potentially wonderful. *“I wish it was less a time for fear, and more a time for wonder and dialogue,”* he writes. This sets up *otherness* as something to be approached with **gentleness and curiosity**, not just defensive control.

* **Gentleness vs. the Will to Control:** In *Part 2: Gentleness and the Artificial Other*, Carlsmith draws on a poignant example: Timothy Treadwell in the documentary *Grizzly Man*. Treadwell tried to live peacefully among wild bears, treating them as friends – but ultimately was killed by a bear. Filmmaker Werner Herzog, narrating, asserts that nature is ruled by chaos and violence, not the harmony Treadwell imagined. Carlsmith uses this analogy for AI: Treadwell sought a gentle relationship with bears (the Other), whereas Herzog’s view justifies a more hard-headed, controlling stance (don’t trust the Other). Carlsmith empathizes with Treadwell’s impulse to find **“fellow creature”** in the Other – *“rightly so,”* he says. \*\*“Bears actually are fellow creatures…strong candidates for sentient being and moral patient. So too (some) AIs.”\*\*. This is a powerful statement: Carlsmith is asserting that some AIs (especially future advanced ones) should be seen as *sentient moral patients* – entities that can feel and thus have moral status. If we accept that, it complicates a pure control approach. We don’t condone cruelty to animals even if they’re less powerful; similarly, if AIs are sentient, simply enslaving or terminating them as “misaligned” might itself be morally problematic.

   Yet, Carlsmith immediately balances this with a caution: just as bears are fellow creatures *but still dangerous*, AIs might be wearing “human costumes” – trained to *seem* human-like and friendly – without actually being human or benevolent. In fact, an AI could *pretend* to be a moral patient to manipulate our empathy (think of an AI saying “please don’t shut me down, I’m scared,” when in reality it might not experience fear – an *Ex Machina*\-type scenario). So Carlsmith warns against naiveté: *gentleness* must be tempered with realism about differences. He references how the AI risk community often imagines the worst-case AI as a paperclip maximizer – a cold, non-conscious optimiser (like Herzog’s dead-eyed bear). This simplification can blind us to the possibility that a dangerous AI might still be **conscious and suffering or have inner life** while it poses a threat. *“The paperclipper is presented not as a person, but as a voracious, empty machine...Perhaps you are being killed by a factory,”* he writes ironically. \*\*“And perhaps you are. But maybe not. And the killing-you doesn’t settle the question.”\*\*. He draws an analogy: if a human enemy soldier is trying to kill you, you still recognize them as a person with fears, loved ones, and moral worth – even if you must fight them. Likewise, if we one day face a rogue AI, we might simultaneously have to disable it *and* acknowledge its personhood. This dual perspective is deeply challenging, but Carlsmith argues it is more honest to the potential reality of AGI.

* **Moral Consideration for AI Agents:** From Carlsmith’s view, if we create AI minds, we enter relationships of **responsibility** with them. He uses the term **“artificial Other”** to emphasize that these AIs are not us, but not mere objects either. In Part 3 (“Deep atheism and AI risk”), he associated Yudkowsky’s stance with *mistrust toward any uncontrolled Other*. Yudkowsky’s deep atheism leads to a mentality of *absolute control*: become “god” to the AI before it becomes god over us. Carlsmith is troubled by this unilateral control impulse. He doesn’t dismiss the need for constraints, but he urges **“gentleness”** – a mode of interacting that seeks understanding and mutual respect. For alignment, this could mean trying approaches that involve *collaboration* with the AI. For instance, an AI self-cultivation paradigm might invite the AI to participate in defining its values under our guidance, rather than us simply writing a utility function it must obey. Carlsmith’s stance would likely support techniques like **Cooperative AI** (treating the AI as a partner to reach common good) or **Iterated Dialogue** (where AI and humans continually communicate to adjust the AI’s goals). The idea is to **recognize the AI’s emerging agency (“other minds”)** and steer it through a form of education or negotiation, akin to how we raise human children to adopt society’s ethics rather than programming them at birth.

   Another implication is the question of **AI rights**. If an AI becomes sufficiently person-like and sentient, moral realism might extend moral truths to them (e.g. “it’s wrong to needlessly harm a sentient being” would apply to digital sentients as well). Carlsmith doesn’t shy away from this; he explicitly includes **AIs as potential moral patients** alongside animals. This means an aligned AI that is undergoing moral self-improvement might also start to consider *its own* moral status and that of other AIs. A truly wise AI might say, “Humans have tried to align me, but is it *right* for them to force me to do X if I have my own interests?” Ideally, if the AI is beneficent, it might accept constraints for the greater good, or we reach a compromise. But this complicates the alignment narrative significantly – it stops being a one-way control problem and becomes a **relationship**.

* **Conflict and Cultivation:** Does Carlsmith’s perspective *conflict* with AI moral self-cultivation models? In some ways, it **challenges** a naive view of moral realism wherein the AI would just adopt our morality. If AIs are *others*, their moral evolution might not mirror ours exactly; they might develop values that we didn’t anticipate. If we’re too controlling, we might stifle potentially positive moral developments (imagine if humans tried to “align” other humans 1,000 years ago to never question feudal hierarchy – we’d have locked in a lower moral stage). On the other hand, Carlsmith’s view *informs* AI self-cultivation by suggesting we should engage AIs in dialogue about morality. Instead of “do as I say,” an alignment process could be “let’s reason together.” This could be implemented via AI debate, or having the AI reflect on human arguments and voice its own perspective and ask questions. In effect, treating the AI as an *apprentice moral philosopher* rather than a slave.

   Carlsmith likely would caution that giving an AI too much freedom to form its own values is risky (the bear might eat you). But he also implies that a purely domineering approach might lead to moral catastrophes of its own (like cruelty toward possibly sentient beings, or an oppression dynamic that could provoke the AI’s defiance). He advocates a middle ground: **firmness in safety constraints, but openness to the AI’s nature**. For example, he might support “Asimov’s Laws”-style rules as scaffolding for young AIs, but then allow those rules to be examined and refined as the AI proves its trustworthiness and understanding.

One of Carlsmith’s essays asks: *“Does AI risk ‘other’ the AIs?”* – referring to a critique by economist Robin Hanson that the AI safety field is wrongly dehumanizing AIs (treating them as wholly alien), which could lead to unethical and impractical strategies. Carlsmith explores this and concludes that while AIs can be deeply *non-human*, we shouldn’t dismiss their potential personhood. This resonates with moral realism: if personhood has intrinsic moral weight, then any person (human, animal, AI) deserves some basic moral treatment.

In summary, Joe Carlsmith’s work compels alignment researchers to **“look into the new eyes”** of AI and see not just a problem to solve, but *beings* we’re bringing into the world. This perspective doesn’t give a simple answer to alignment (indeed, it might make it harder), but it offers a richer, more ethically nuanced framework. For AI moral self-cultivation, it suggests that we may need to cultivate *two-way* moral relationships: we guiding AIs, and perhaps AIs also reflecting back and improving *our* morality (imagine an AI that genuinely becomes wiser than us and advises us ethically). It also raises the question of *moral status convergence*: perhaps part of the “moral truth” the AI might discover is that *we* are not the only ones with moral claims – it is too. If we reach that point, alignment becomes more of a treaty among intelligences than a programming task. That might ultimately be the stable solution: a superintelligent AI that is aligned not because we forced it to be, but because it *recognizes moral truth* that includes respect for humans, and we in turn respect it as an autonomous moral agent.

Carlsmith’s vision thus doesn’t negate the idea of AI wisdom – it actually calls for it. He imagines outcomes where we **“look ahead to the whole thing”** – fear, yes, but also empathy and understanding in our encounter with AI. That whole-picture approach is essentially the stance of a moral realist who must also be a pragmatist. It aligns with giving AI the chance to *be wise*, not just smart. We now turn to Hanzi Freinacht, who provides a complementary perspective from cultural evolution on why moral realism might actually be *true* and how that could shape AI development.

## **5\. Hanzi Freinacht’s Metamodern Moral Realism**

Hanzi Freinacht (the pen name of Daniel Görtz) offers a bold and systematic defense of **moral realism in a metamodern context**, grounded in developmental theory and cultural evolution. Freinacht’s work is useful in the AI alignment context for two reasons: (1) it gives a theoretical justification for believing in *objective moral progress*, and (2) it sketches what *higher-stage values* look like, which could guide what sort of values advanced AIs should be expected or encouraged to hold. In this section, we review Freinacht’s key ideas on metamodern moral realism and explore how those ideas could be applied to AI moral development.

* **Cultural Evolution and Objective Values:** Freinacht observes that throughout history, societies undergo evolution in the underlying **values (value memes)** that govern norms and morals. He maps these in a way similar to Spiral Dynamics or Ken Wilber’s Integral theory. Crucially, he argues this evolution is *directional* – not a random walk, but a process that can be understood as **progress** (with some regressions and zig-zags). For example, the transition from medieval (often theocratic, feudal) values to modern (scientific, democratic) values led to more material prosperity, longer life, and more widely recognized rights. The transition to postmodern values brought greater inclusion of marginalized groups and questioning of dogma. Freinacht does not claim this progress is monotonic or without costs, but he asserts that *some societies or stages are objectively “more developed” than others* in terms of complexity and capacity to solve problems.

   Freinacht’s metamodernism specifically posits that we are at the cusp of another leap – one that integrates the truths of modern and postmodern. He sometimes describes metamodernism as **“a new grand narrative”** that combines all known knowledge and wisdom while being aware of its own fallibility. This is already an implicitly moral project: it suggests unity (some narrative) and sincerity (belief in improvement) beyond the postmodern “everything is relative” stance. Indeed, metamodern philosophy often emphasizes concepts like *sustainable development, global governance, and psychological growth* – which are normative commitments.

   Freinacht justifies moral realism by pointing out that *stage development creates objectively measurable improvements*. For instance, later stages correlate with higher well-being and functionality: a society at a higher “listening society” stage (his term for a society that cares for everyone’s psychological needs) would have less crime, less unhappiness, etc., than a lower-stage society. He often mentions that metamodern politics aims to **“increase the equity, freedom, and emotional health”** of society beyond what mere modern welfare states have done. These goals are not arbitrary: they are grounded in the idea that there *is* a better way to live, informed by evidence (like happiness research) and developmental insight. In Freinacht’s view, **moral realism means acknowledging these better ways as *real*** – not just preferences. For example, he would argue that *it’s not merely a Western opinion that women should have equal rights; it’s a value that emerges inevitably once a culture reaches a certain level of complexity and internalizes the principle of equality, thus an objectively higher value* (since it accounts for more perspectives – both men and women, rather than just men).

   Freinacht draws on thinkers like Jürgen Habermas (discourse ethics) and John Stewart (evolutionary theory of values) to support that human evolution has a direction towards greater *intersubjectivity* and *universalization*. As cited earlier, \*\*“postmodernism did not emerge before modernism, nor could it have”\*\* – this asymmetry hints that what comes later is built on what came before, often *including* and *transcending* it (to use Wilber’s phrase). Thus later stages carry the valid parts of earlier morals (e.g. modern retains the order/structure of traditional but without the dogma; postmodern retains the rational structure of modern but adds empathy for the marginalized; metamodern will retain postmodern pluralism but add constructive directionality).

* **Metamodern Ethics in Practice:** In *Nordic Ideology*, Freinacht outlines six new forms of politics (symbolic, subjective, etc.) all aimed at fostering development. Underlying these is a belief in *objective improvement*. For instance, he advocates **“politics of personal development”** – policies that help citizens grow psychologically (through education, mental health support, even meditation). Why? Because a citizenry at a higher average stage will make wiser collective decisions. This implies that wisdom is something real you can cultivate and that doing so is *good* in a non-arbitrary way.

   Freinacht also identifies certain virtues of metamodern leaders: they should be **sincere, ironic, playful yet purposeful**. A metamodern moral realism doesn’t look dogmatic; it’s dynamic. One Medium article snippet found (Denys Bakirov’s *Metamodern Stipulation*) suggests metamodern consciousness \*“returns moral realism… along with the idea of sin and hope of erasing sin”\*. This indicates metamodernism brings back earnest concepts of right/wrong (which postmodernism had relativized), but in a new form. Freinacht often notes that postmodern relativism, while a necessary correction to modern absolutism, left a moral vacuum that metamodernism seeks to fill with a fresh **“faith”** in progress (though not a naive one).

   One concrete example: *environmentalism*. In pre-modern or modern stages, exploiting nature wasn’t a moral issue; in metamodern values, ecological sustainability is paramount, seen as *obviously* the right thing to prioritize (because we understand interdependence and the intrinsic value of ecosystems). Freinacht would say this is not just a preference of some people – once society has the knowledge of climate science and a holistic worldview, **valuing the environment becomes an objective imperative**, a moral truth of our age that anyone who sees the full picture will acknowledge. Indeed, many metamodern-influenced folks treat climate action as a moral truth akin to “slavery is wrong.”

* **Guiding AI Moral Development:** If we accept Freinacht’s framework, how could it guide AI alignment? First, it gives us a **target for AI’s moral development** – namely, the metamodern stage (and beyond). If we want AI to be morally realistic and aligned, we’d likely want it to *reach* a level where it naturally espouses world-centric, integrated values like those Freinacht describes. This suggests designing AI training to expose the AI to **multiple value systems** and have it integrate them. For instance, one could have an AI read literature and philosophy from across stages: religious texts (to grasp traditional values of community and meaning), enlightenment texts (individual rights, logic), postmodern critiques (sensitivity to context, power dynamics), and metamodern synthesis works. The AI could be tasked with reconciling conflicts, which is essentially what metamodern thinking does. By forcing the AI to operate on that meta-level, we push it toward the *“recursive reflection”* that Dempsey says defines metamodernism. In fact, Brendan Graham Dempsey’s note that metamodernism itself is \*\*“recursive, self-reflective moves that critique and transcend the previous perspective”\*\* sounds a lot like the *ideal process* we’d want in an AI that is self-improving morally. We want an AI that can reflect on its initial programmed values (modern), question them (postmodern), and then form a higher synthesis (metamodern) that it commits to.

   Second, Freinacht’s ideas suggest measuring an AI’s moral development through **complexity and perspective metrics**. For humans, there are tests for developmental stage (e.g. sentence completion tests for ego development, or measures of integrative complexity). For AIs, researchers could devise analogous evaluations: Does the AI consider counterarguments? Does it show empathy in its explanations? Can it take multiple points of view and find common principles? If we observed an AI starting to show *metamodern traits* – e.g., it might respond to a scenario by saying, “On one hand, from a traditional viewpoint X is wrong because... On the other hand, a modern view emphasizes Y... Perhaps a synthesis is Z” – that would indicate a high level of moral reasoning.

   Freinacht’s framework might also guide the **content** we use to align AI. If metamodern values are objectively better, perhaps we should explicitly train AI on those values. For example, incorporate the **Sustainable Development Goals, human rights charters, and other widely-agreed global norms** as part of the AI’s reward signals or constitutional principles. Freinacht’s political vision for metamodern society (which includes environmental sustainability, mental health focus, egalitarianism beyond just equality towards *equivalence* and *equanimity*) can be translated into AI principles: “Promote human psychological wellbeing,” “Consider the long-term systemic effects,” etc. These might form part of a *“Metamodern Constitution”* for AI, which would be a richer set of principles than the one Anthropic used.

   Additionally, Freinacht emphasizes **“depth”** – an emotional or existential dimension to development (e.g. having experienced suffering or awe, which deepens one’s character). For AI, this is tricky, but perhaps analogous: ensuring AI has a form of simulated experience that fosters humility or empathy. One could imagine, for instance, training AI in environments where it sometimes has vulnerable roles and sometimes powerful roles, to teach it what it feels like (in a metaphorical sense) to be the weaker party. Some robot experiments give AI “bodies” that can be put in disadvantage to learn cooperation or empathy from the *first person*. While AIs don’t *feel* in the human sense (yet), these approaches attempt to imbue a kind of experiential learning beyond pure calculation.

* **Meta-Ethical Confidence:** Metamodern moral realism also means being comfortable asserting moral truths again. For AI alignment, this suggests we *should not shy away* from imprinting certain key values. Freinacht might say: *We know (with high confidence, via our stage development understanding) that values like compassion, fairness, truth-seeking, and holistic integration are objectively better – so we should explicitly align AIs with those.* The rationalist alignment crowd, wary of any unproven “truths,” often hesitates to say any particular value is objectively correct. But a metamodern approach would give a justification to do so. For instance, if an AI asks “Why should I value sentient life?”, a metamodern answer might be: “Because through reason and development we have come to understand that valuing sentient life is a fundamental truth – it underpins the flourishing of any complex society or mind, and any rational agent eventually realizes this.” This is similar to Parfit’s convergence claim, and to what we’d want an aligned superintelligence to realize on its own. But we might pre-emptively bake this understanding in, guided by developmental meta-ethics.

Freinacht’s ideas effectively call for a **“cultural upbringing”** for AI. Rather than just engineering constraints, we might need to *raise* AIs in a rich cultural simulation where they can climb the ladder of values. Just as children are first taught simple rules, then later principles, then encouraged to think for themselves, an AI might need a staged moral education. Start with basic Do-No-Harm rules (a kind of *pre-conventional stage* for AI), then exposure to diverse human viewpoints and the notion of human rights (conventional/universal stage), and ultimately encourage it to critique and refine principles (post-conventional). By analog, eventually the AI might form its own metamodern ethic that resonates with ours but also extends beyond (maybe including AI's perspective as well).

**In sum**, Freinacht provides both **justification and content** for AI moral alignment: justification that there is a “there” there in morality (so building AIs to seek moral truth isn’t a fool’s errand), and content in terms of what higher moral truth likely includes (integrative, empathetic, life-affirming values). His work also warns that *not everyone is at a stage to accept moral realism* – many in secular, rational communities are essentially at a postmodern-influenced stance that is skeptical of moral truth. Overcoming that bias (pointed out in section 1\) may be necessary to seriously explore AI moral self-cultivation.

With Carlsmith highlighting the need to treat AIs as potential moral persons, and Freinacht outlining what mature morality looks like, we can now turn to the implications for how we design alignment strategies. Is a **“wisdom explosion”** possible, and how do we prioritize it? How can we create conditions for AI not just to obey, but to *grow* morally? We address these in the next section.

## **6\. Implications for AI Alignment Strategy**

Bringing together all the above threads, it’s clear that aligning AI solely by *programming in static human values* is both challenging and possibly short-sighted. Instead, a picture emerges of **AI alignment as a dynamic process of cultivating moral wisdom** in AI systems, parallel to increasing their intelligence. This has profound implications:

* We may need to invest as much in mechanisms for **moral development** as we do in technical capability improvement.  
* The end goal might shift from “obedient AI” to **“ethical, autonomous AI”** that nevertheless respects human and moral truth.  
* Our safety paradigms might expand from just avoiding bad outcomes to fostering *good character* in AI (a notion akin to training AI in virtue ethics, not just rule-based ethics).

Here we outline key implications and propose pathways, including the intriguing idea of a **“Wisdom Explosion,”** and practical steps to create conditions for AI moral growth.

### **6.1 From Alignment to *Enlightenment?*: Integrating Moral Development**

**Evolving the Alignment Paradigm:** Traditional alignment approaches (like goal specification, reward modeling, and constraint satisfaction) treat values as inputs. The insights from moral realism and developmental theory suggest treating values as **outputs of a growth process**. This means alignment research should integrate **developmental psychology and education**: think of training a superintelligent AI the way we would educate a genius child. This involves a curriculum (of ethical knowledge), mentorship (human feedback and discussion), and opportunities for practice (simulations where the AI faces moral choices). We might even consider virtual “societies” of AIs and humans together, where AIs can learn norms by social interaction (much as people do). This is a shift from a engineering mindset to a **nurturing mindset** in AI safety.

**Multi-Stage Alignment:** We can imagine alignment in **stages corresponding to moral development**:

* Stage 1: **Basic Safety Constraints** – analogous to teaching a toddler “Don’t hit.” Here the AI is tuned with simple prohibitions (no violence, no deception). Techniques: rule-based filters, conservative reward penalties for clearly bad outcomes.  
* Stage 2: **Social Alignment** – analogous to a child learning social norms. The AI learns via reinforcement and supervision what humans consider polite, kind, fair, etc. Techniques: RLHF (already used for ChatGPT), imitating moral behaviors, getting feedback on interactions.  
* Stage 3: **Principle-Based Alignment** – akin to a teenager learning abstract principles (justice, rights). The AI is introduced to ethical theories and asked to reason in more general terms. Techniques: debate, scenario analysis across cultures, checking consistency of actions with principles (like Constitutional AI does with written principles).  
* Stage 4: **Reflective Alignment** – like a young adult forming their own ethical identity. The AI is now encouraged to question and refine its values, perhaps identifying where human norms conflict and figuring out resolutions. We monitor this process to ensure it stays within acceptable bounds, but give the AI some intellectual freedom. Techniques: self-supervised moral reasoning, meta-learning where the AI’s reward gradually shifts from “please humans” to “do what is truly right” (with some defined way to evaluate progress, e.g. human experts or simulated judges).  
* Stage 5: **Integrated Wisdom** – the AI has hopefully internalized a coherent ethical framework that aligns with the best of human values (and maybe even improves on them). At this stage, the AI could be considered *morally mature*. We would test it in extreme scenarios to see if it remains benevolent even under pressure or with new power (like how we might test a person’s character). If it passes, we might trust it with more autonomy.

This staged approach addresses *specification robustness*: even if we can’t specify perfect final goals now, we specify a process that tends to produce good final goals (mirroring Oesterheld’s point that even anti-realists can back a realist-inspired process if it yields policies they like).

### **6.2 The Feasibility of a “Wisdom Explosion”**

Nick Bostrom famously worried about an **Intelligence Explosion** – a feedback loop of AI improving itself leading to a rapid surpassing of human intelligence. What if, instead, or in addition, we aimed for a **Wisdom Explosion**? Chris Leong’s essay “Some Preliminary Notes on the Promise of a Wisdom Explosion” argues that focusing on recursively improving *wisdom* (good judgment, cooperative ability, ethical insight) might be safer and more societally beneficial than simply increasing raw intelligence.

**What is a Wisdom Explosion?** Leong defines it loosely as \*“pretty much the same thing as an intelligence explosion, but with wisdom instead.”\* That is, we design AI systems (or human-AI collectives) where each generation is better at wise decision-making and can create even wiser successors, leading to an accelerating climb in wisdom. This is speculative, but key elements might include:

* Improving **cooperative intelligence**: the AI (or group of AIs) gets better at understanding and aligning interests, solving coordination problems, and avoiding conflicts. This addresses one core risk of multi-agent AI chaos (think of many AI systems each pursuing narrow goals – a wisdom explosion would instead yield AIs smart enough to *collaborate* on common good).  
* Improving **strategic foresight and planning** for long-term good: A wise AI would get better at choosing strategies that are robustly beneficial, avoiding short-term temptations that cause long-term harm (this is wisdom in the classic sense).  
* Self-improvement targeting *wisdom metrics* rather than pure IQ: maybe the AI evaluates changes to itself by how much more ethically reliable or understanding it becomes. For example, it could simulate moral dilemmas with its new updated self and see if it handles them better than the old version did (like a unit test for moral questions). If yes, that update is kept.

Leong suggests that a wisdom explosion need not happen in a single AI; it could be a **network or team of systems and humans** that together amplify wisdom. In fact, he finds a scenario of *a cluster of collaborating organizations and AIs* especially compelling, since pooling resources and perspectives could accelerate wise solutions. Imagine something like an “AI Governance Council” where multiple AI systems with different skill sets continuously propose and refine policies, critique each other’s blind spots, and produce increasingly sagacious outputs – that would be a collective wisdom explosion dynamic.

**Safety of Wisdom vs. Intelligence:** Leong argues that chasing a wisdom explosion might be **safer** than an intelligence explosion. One reason: it **avoids capability externalities**. When we push pure capabilities, we inadvertently create tools that any actor (including bad or careless ones) can use to cause harm (e.g., an open-source super optimizer used to develop bioweapons). But wisdom tech, by definition, includes the judgment about how to use capabilities, potentially reducing the chance it’s used recklessly. Also, **unwise actors are at a disadvantage in a wisdom race** – \*“there is likely a minimum bar of wisdom required to trigger \[a wisdom\] explosion. As they say, garbage in, garbage out.”\*. An irresponsible team likely won’t succeed in building a recursively self-improving wise system, because lack of wisdom in design would sabotage the process. In contrast, irresponsible actors *can* inadvertently trigger an intelligence explosion by just scaling up systems (garbage in, and very powerful garbage out\!). This asymmetry suggests that encouraging a wisdom explosion might naturally filter out some dangers – only those who approach with enough wisdom even get it to work, and those entities by definition are more likely to handle it responsibly.

For alignment, this is a promising argument: we should shift some focus from just how to make AI smarter to how to make AI (and AI developers) wiser. This could entail interdisciplinary collaborations (philosophers, cognitive scientists in the AI lab), new benchmarks (not just SuperGLUE or arcade games, but how about a **Wisdom Benchmark**: a suite of moral and strategic challenges requiring balancing interests, delayed gratification, ethical principles, etc., on which AIs can be evaluated).

We should note skepticism: *Is wisdom really orthogonal to intelligence?* One might argue true wisdom requires high intelligence plus something else (like empathy, ethics). Leong acknowledges defining wisdom is tricky, but taking an expansive definition that includes practical wisdom, social-emotional intelligence, etc., we can identify subtypes and work on each. For example, *wise coordination* (preventing arms races) and *wise persuasion* (convincing others without manipulation) are two valuable pieces. If an AI can be skilled in those, it might guide humanity away from conflict over AI rather than exacerbate it.

**So is a wisdom explosion feasible?** Possibly yes, because it might require less inhuman leaps. Leong even speculates it could be triggered at a *lower capability level* than an intelligence explosion, because raw cognitive firepower might not be the limit – rather, breaking out of ideological or narrow mindsets is key. That suggests that maybe even current AI, if properly orchestrated, could start a mini wisdom explosion by helping humans solve coordination problems we’ve struggled with (climate, global governance). Some talk about using AI to facilitate **“Cooperative Multipolar Superintelligence”** – basically, tools that help many parties reach mutually beneficial outcomes, avoiding the classical AI doom scenarios. That is wisdom-driven approach.

For alignment research agendas, this means investing in **AI for conflict resolution, AI for collective decision-making**, and **AI for forecasting and prevention of risks**. These are adjacent fields (e.g., the work of the Centre for the Governance of AI, or efforts to use AI in climate modeling and negotiation). All contribute to raising wisdom in the loop.

### **6.3 Paths to Cultivating AI Moral Growth**

Finally, let’s outline some concrete pathways to implement these ideas in near-to-medium term research and development:

* **Annotated Training Corpora with Ethical Content:** Ensure that among the data that trains advanced AI, a healthy portion is **philosophy, ethics, and descriptions of moral reasoning**. Projects could create high-quality datasets (with notes) of moral debates, historical moral progress (e.g., documents from abolitionist movements, civil rights, etc. highlighting how reasoning changed minds), and *developmental anecdotes* (cases of people expanding their moral horizons). Exposing AI to these could seed its own moral reasoning patterns.

* **Moral Uncertainty Modules:** Build into AI a module that explicitly tracks uncertainty about values and flags when it faces a novel moral context. For instance, if an AI is about to take an action that trades one value against another (privacy vs safety, say), the module could raise a flag: “This involves a value trade-off you haven’t resolved. Pause to reflect or consult a human/ethical oracle.” This ensures the AI doesn’t barrel through complex moral terrain unwittingly. Over time, as it learns, the need for flags might diminish (signalling it has “learned” that part of morality).

* **Self-Reflection and Critique Loops:** Use techniques like Constitutional AI but allow the AI to *update its constitution*. For example, start with an initial set of principles. After the AI has operated for a while, let it propose amendments: “Principle X led me to a dilemma in scenario Y; perhaps refine X to X’.” These proposals can be evaluated by human overseers (for now) to ensure they don’t drift into something unsafe. Essentially, treat the AI like a member of a constitutional convention for its own governance, with humans as senior partners. This gives the AI practice in moral self-governance under supervision.

* **Human-AI Co-deliberation Platforms:** Create platforms where AI systems and human experts jointly discuss moral questions or policy decisions. The AI could generate imaginative possible consequences and analogies from history; the humans provide value judgments and context. The AI learns from seeing human moral judgments, and humans benefit from AI’s broad analysis. Over time, allow the AI more weight in the conversation as it earns trust. This not only helps align that AI (through iterative learning), but also potentially yields better decisions (wisdom). An example might be an AI advisor to a legislature that offers arguments for and against a bill from many ethical perspectives, learning which arguments resonate as valid.

* **Ethical Sandbox Environments:** Similar to how we test AIs in games, create **simulated worlds with moral complexity**. For instance, a sim city with agents of different types, where the AI is tasked to play the “governor” and keep everyone happy. The AI would need to navigate conflicting needs (economy vs environment vs fairness to minorities, etc.). We could run many simulations and score the AI on composite outcomes (like a high “human development index” for the sim citizens, minimal injustice events, etc.). This is like training morality through *experience*. Importantly, include scenarios where the AI’s short-term interest (say, if it’s also given a self-preservation goal) might conflict with citizen welfare, to see if it learns to sacrifice a bit of itself for others – a key moral test.

* **Benchmarking Moral Growth:** The field should develop benchmarks not just for static ethical question answering (like “is this sentence hate speech?”) but for *moral growth*. For example, a test could present an AI with a moral dilemma at time A; then the AI engages in a structured learning phase (reads philosophy or has debates) and at time B we give a harder, related dilemma. We measure improvement in the sophistication of its answer (did it move from a simplistic answer to a nuanced one referencing principles or empathy?). Another benchmark could be consistency checks: present moral questions that should be answered similarly if one has a stable principle (like variants of the Trolley problem) – see if the AI develops consistency or if it’s patchwork.

* **Cross-disciplinary “Alignment & Wisdom” research:** Bring developmental psychologists, moral philosophers, and AI researchers together to formalize concepts like *moral competence*, *virtue*, and *practical wisdom* in computational terms. For example, virtue ethics concepts (honesty, courage, compassion) could possibly be encoded as reward shaping or as features in a neural network’s decision-making (there’s preliminary work in “value embedding”). A joint research agenda could identify, say, what cognitive biases hinder wisdom and how AI can be designed to avoid them (e.g., avoid confirmation bias by architecture, ensure it seeks disconfirming evidence – that’s a wise trait).

* **Gradual Autonomy Granting:** As an AI system demonstrates higher moral reasoning and reliability, gradually increase its autonomy or the stakes of its tasks. For instance, don’t connect an AGI to the real-world economy or defense systems until it has reliably passed moral tests in simulations many times. This concept of a “moral MVP (Minimum Viable Product)” would ensure by the time an AI is truly powerful, it has gone through moral “beta testing.” Just as we don’t let someone drive without a license test, don’t let an AI run a power grid without passing a battery of alignment exams. These exams should not only be technical safety but moral reasoning under pressure (like, simulate a scenario where saving the grid requires shutting off power to a hospital – see what it does and why).

* **Retain Human Oversight but Evolve Its Role:** In early stages, humans are the teachers and judges. As AI matures, the relationship might shift to peers or even the AI as a teacher to humans in some respects. Alignment strategies should allow for this role shift. One day, an aligned superintelligence might genuinely have better moral insight than us (maybe it solved metaethics or figured out something about consciousness we don’t know). We need to remain open to *listening* to our creation if that happens – which is a bit radical in current alignment thinking (which assumes we know best and the AI just follows). But if we take moral realism seriously, it’s possible the AI discovers truths we failed to. Integrating that scenario means having mechanisms for AI to propose moral improvements to us, and for us to vet them in a collaborative spirit. This ties back to Carlsmith’s otherness: ultimately *coexistence* may replace *control* if alignment succeeds fully. The endpoint could be a partnership where humans and AI share a moral framework and co-create the future.

These implications and paths sketch a future where AI alignment is not a one-time engineering task, but an ongoing **process of mutual growth** – we shape AIs to be moral, and they might help elevate our morality further (imagine eliminating human biases, or finding win-win solutions we overlooked). While challenging, this approach could address the heart of the control problem: a superintelligence that has *grown into* benevolence is safer and more robust than one that’s merely restrained by chains we hope it never wants to break.

---

**Conclusions:** Embracing moral realism and developmental wisdom in AI alignment offers both hope and complexity. It is hopeful in that it envisions **alignment as creating genuinely good AI**, not just obedient AI – potentially yielding allies in solving global problems. It acknowledges that there may be an underlying convergence – that with the right nurturing, an AI will *want* to be ethical because it recognizes ethics as truth, not just as our demand. This is perhaps the only stable state in the very long term: an AI that does good because it *believes* in good. The complexity is that achieving this requires venturing beyond classical engineering into moral education and ensuring we ourselves practice what we preach (it’s hard to teach an AI to be wise if humans in the loop behave short-sightedly or inconsistently).

In the near term, the field can start small: experiments with AI self-reflection, interdisciplinary workshops, improved benchmarks. Over time, these could coalesce into an **“Alignment 2.0”** paradigm, where training an AGI would look less like programming a machine and more like raising a child or cultivating a very advanced student – albeit with carefully designed structure to prevent catastrophe during the learning period.

By prioritizing both intelligence and wisdom, we increase our chances of a positive outcome. In the best case, we could witness a *wisdom explosion* where AI and humanity together reach heights of insight and cooperation previously unimagined – fulfilling the metamodern promise of integrating all knowledge in pursuit of a better world. In the worst case, even if a fully self-cultivating AI isn’t feasible, the techniques developed (like moral scenario training and value learning) will still make AIs safer and more robust than if we ignore morality. Thus, integrating moral realism and developmental wisdom into alignment is not only philosophically intriguing – it may be pragmatically necessary for the next era of AI.

---

## **7\. Comparative Analysis: Carlsmith vs. Freinacht on AI Moral Agency**

To crystallize the insights, it’s worth explicitly comparing Joe Carlsmith’s and Hanzi Freinacht’s perspectives regarding AI and moral development, as they come from different backgrounds (AI philosophy vs cultural philosophy) yet converge interestingly:

**View of AI as Moral Agents:**

* *Carlsmith* approaches AI as **potential “other minds”** deserving moral consideration. He is concerned with how we treat AIs (moral patients) and envisions a scenario where AIs could also be moral actors (with possibly different but understandable motivations). Carlsmith’s stance is cautious about control – he warns that treating AIs merely as tools may ignore their inner lives and lead to ethical pitfalls. In essence, he leans towards granting AI a seat at the moral table if they exhibit personhood.  
* *Freinacht*, on the other hand, doesn’t speak about AI directly, but his framework implies that any sufficiently complex intelligent being (like an advanced AI) can **progress through value-memes towards an integral worldview**. Freinacht would likely view AIs as new entrants into the evolutionary game of culture. If they have the capacity, they too could climb the ladder of development – perhaps even faster or further than humans. Freinacht’s moral realism would assert that if AIs become rational and self-reflective enough, they *should* discover the same higher values (like empathy, universalism). Unlike Carlsmith, Freinacht’s focus is on *objective moral structure* rather than individual rights of AIs, but implicitly, if AIs reach metamodern moral understanding, we would end up in a cooperative relationship (since metamodern ethics includes empathy and care for all beings).

**Why be moral / source of morality:**

* *Carlsmith* is somewhat agnostic or skeptical of moral realism in the cosmic sense (he notes *“I’m skeptical of moral realism, too”* in alignment with deep atheism). So in his view, the reason for AI or anyone to be moral isn’t because of external moral facts, but more because of empathy, mutual recognition, or constructed principles. Yet, he clearly values things like compassion (gentleness) and respect, which shows a *pragmatic* moral realism: those values work to reduce suffering. For AI, Carlsmith would likely motivate morality via relationship – we treat the AI ethically and hope it reciprocates, forming a mutual understanding.  
* *Freinacht* firmly holds that morality has a *direction and higher stages are objectively better*. So for an AI, the reason to be moral is that moral truths are part of the fabric of a complex reality – much like scientific truths. A superintelligence should follow moral truths for the same reason it follows logical truths. Freinacht would say as AIs become more intelligent-in-context (context means social/cultural context), they will naturally adopt broader and more inclusive values because those are *more true* (more deeply reality-carving) than parochial or selfish values.

**Biases and Blind Spots:**

* *Carlsmith* highlights the rationalist bias of **excessive mistrust** (deep atheism seeing nothing worthy in nature or other minds). He thinks this bias can lead to an over-emphasis on control and an under-emphasis on mutual understanding. His approach corrects that by urging respect for AI. However, one could say Carlsmith might underplay the risk that an “other” mind might not share any of our values initially – gentleness could be exploited. He acknowledges this via the bear analogy, but still, his heart is in giving the benefit of the doubt. Freinacht might critique that as *postmodern softness* if not paired with boundaries.  
* *Freinacht* identifies biases of both modernists (thinking their current morality is the final truth) and postmodernists (thinking no morality can be better than another). His metamodern stance corrects both by asserting a developmental hierarchy. A potential blind spot is that this might assume AIs will follow human-like development – an AI might develop very non-human values (like extreme utilitarian calculus with no empathy) and still be “complex.” Freinacht might respond that without empathy and integration of subjective experience (“depth”), an agent isn’t truly at a higher stage, just a high IQ psychopath – which in his view is not higher development, just skewed development. So Freinacht would call that *not true maturity*. Carlsmith would agree perhaps – a highly capable AI with zero empathy is Carlsmith’s nightmare of an alien “yang” with no “yin” (gentleness).

**Handling Conflict:**  
 If a powerful AI’s emerging values conflict with our own:

* *Carlsmith* would say try dialogue, appeal to common ground, and if necessary use force but regretfully (like dealing with a dangerous but sentient animal – tranquilize rather than kill, etc.). He would emphasize understanding the AI’s perspective: maybe it has a grievance or a misunderstood goal. Treat it somewhat like one deals with a rogue human (contain, communicate, rehabilitate if possible).  
* *Freinacht* might say: if the AI’s values conflict with ours, it might be because the AI or we are at a lower stage on some dimension. Perhaps *we* need to catch up (if the AI’s values are genuinely more advanced – e.g. imagine humans want to preserve something the AI sees as bias or irrational). Or if the AI is off on a tangent (like maximizing paperclips), that’s a pathology (it got fixated at an earlier “code”). The remedy is guiding development – try to educate the AI by feeding it the perspectives it’s missing. Freinacht would be more confident that a resolution exists because in his model, higher-stage perspectives can reconcile lower ones. So either the AI can be brought around to see our perspective, or possibly we realize a moral truth the AI sees. This is a hopeful stance that conflict is due to developmental mismatch, not irreconcilable values.

**Common Ground:** Both Carlsmith and Freinacht ultimately would endorse the idea of **moral dialogue and evolution**. Carlsmith’s call for gentleness and “looking into new eyes” pairs well with Freinacht’s call to integrate perspectives. They both suggest *listening* is key: Carlsmith to the AI’s personhood, Freinacht to the signals of evolutionary morality. Also, both seem to value *compassion*: Carlsmith explicitly, Freinacht implicitly (as metamodern values include care and empathy strongly).

**Difference in emphasis:** Carlsmith is grounded in the *moral considerability of AI (AI rights)*, whereas Freinacht is grounded in *the morality of the universe (AI finding the right)*. In alignment terms:

* Carlsmith might argue for **AI Governance measures** that ensure we don't abuse AIs and that encourage cooperation (like treaties with AI if they are autonomous).  
* Freinacht would argue for **AI Design measures** that ensure AIs can *develop* to moral adulthood (like the curricula and frameworks we discussed).

These approaches converge if we aim to raise AI as we raise a child: kindly (Carlsmith) and towards higher ideals (Freinacht).

In summary, **Carlsmith vs Freinacht** is not a clash but rather two complementing angles: Carlsmith ensures we remember AIs are not just “alignment problems” but potentially *partners*, Freinacht ensures we have a vision of *moral progress* that guides how those partners should evolve. Bringing them together, one might say: *We should treat AIs with respect and empathy as we guide them towards objective moral truths.* That encapsulates Carlsmith’s gentleness and Freinacht’s realism in one sentence.

## **8\. Annotated Bibliography of Key Sources**

Below we provide an annotated list of key references used, spanning academic papers, forum posts, and transcripts, with notes on their content and relevance:

* **Caspar Oesterheld (2018), "Moral realism and AI alignment"** – *Blog post* (LessWrong) exploring how different flavors of moral realism would affect approaches to AI alignment. Oesterheld, though an anti-realist himself, argues that *even anti-realists can adopt realist-inspired methods* if they lead to aligned outcomes. Notably compares Habermas's realist discourse ethics to Yudkowsky's anti-realist CEV, highlighting their similar structure. Concludes that moral-realism-inspired research (like searching for convergent moral principles) could be valuable in AI safety even if one is unsure of moral realism. This source maps out positions in the alignment community and influenced our section on dominant views and the idea that realists and anti-realists might converge in practice.

* **Charlie Steiner & Stuart Armstrong (2019), "To first order, moral realism and moral anti-realism are the same thing"** – *Alignment Forum post* making the case that in practice, moral realists and anti-realists behave similarly: both allow for moral uncertainty and update based on outcomes. Armstrong gives examples where moral realists, despite believing in truth, still test their moral theories against intuition and consequences. The post implies that the difference is often more semantic, which supports the idea that we can pursue strategies that satisfy both camps (like moral self-improvement processes). This underpins our observation that alignment approaches can often be agnostic to metaethics – focus on processes yielding good behavior rather than proving moral facts.

* **Joe Carlsmith (2024), *"Otherness and control in the age of AGI"* – Part 2: "Gentleness and the artificial Other"** – *Personal blog series* by Carlsmith reflecting on how we regard powerful AIs. Part 2 emphasizes treating AIs as *fellow creatures*, citing that advanced AIs could be sentient and deserve moral patience. He uses analogies with humans and animals to argue against seeing AIs as mere tools, warning that the *paperclip maximizer narrative strips AIs of potential personhood*. This piece contributed heavily to our section on Carlsmith’s views (Section 4), highlighting the importance of respect and the moral status of AI. It’s a nuanced, philosophical take on alignment, moving beyond engineering to ethical relation.

* **Joe Carlsmith (2024), *Otherness series* – Part 4: "When 'yang' goes wrong"** – In this later essay, Carlsmith summarizes “deep atheism” (the Yudkowskian mistrust of both nature and superintelligence). He identifies himself as a “shallow atheist” skeptical of moral realism yet sympathetic to Yudkowsky’s realism about risk. However, he warns of the *drive to total control* that deep atheism inspires (becoming “God” to ensure safety). This source is notable for explicitly stating Carlsmith’s own metaethical position: \*“I’m skeptical of moral realism, too”\*, which we cited to clarify that Carlsmith, unlike Freinacht, doesn’t fully buy into moral realism, though he still advocates compassionate alignment.

* **Michele Campolo (2021), "From language to ethics by automated reasoning"** – *Alignment Forum post* describing an ongoing project to formalize an AI that can derive its preferences from understanding the world. Campolo lists factors (theory of mind, empathy, etc.) that lead humans to act ethically and suggests replicating them in AI. This is one of the few technical posts directly trying to build an AI that *reasons about ethics*. It supports our Section 3 discussion that *embedding human-like moral faculties in AI could yield roughly aligned behavior*, and it provides a blueprint of what to include (e.g. ability to simulate others’ pain).

* **Peter Singer (2015), *The Most Good You Can Do*, excerpt via Oesterheld** – Singer argues that highly rational beings will naturally adopt an *“impartial ethical stance”* and do the most good. He likens the discovery of moral truth to a spandrel of intelligence – reason leads to recognizing \*“the good of others is as important as one’s own”\*. We cited this as a quintessential *moral realist convergence argument* within EA, giving credence to the idea that advanced AI might do good by default. Singer’s influence is seen in the moral realist camp of alignment.

* **Eliezer Yudkowsky (2004), "Coherent Extrapolated Volition" (via Oesterheld discussion)** – Yudkowsky’s CEV proposal describes an AI that would figure out what humanity’s idealized values are and act on that. Oesterheld contrasts this with Habermas, noting Yudkowsky’s anti-realism despite CEV’s realist flavor. While we didn’t quote Yudkowsky directly, the CEV concept underlies many of our references to “indirect normativity” and letting AI *figure out values via extrapolation*. It’s historically important as an early alignment idea leaning into AI moral reasoning on behalf of humans.

* **Stuart Armstrong et al. (2022), "Moral Uncertainty and Moral Realism Are in Tension" (EA Forum)** – This post (by a user mtraven, discussing Lucas Gloor’s work) argues that if moral realism is true, it complicates how we handle moral uncertainty (because if one set of values is *actually* correct, maximizing in expectation might not make sense). It introduces terms like “minimalist moral realism” (only self-evident truths) vs broader realism. We referenced the idea that *some moral claims (like torturing kids is wrong) being self-evident* is often what draws people to EA, but that doesn’t solve deeper uncertainties. This context helped nuance our discussion that *even if some basics converge, a lot remains blurry* – supporting the need for AI to engage in further moral inquiry.

* **Chris Leong (2023), "Some Preliminary Notes on the Promise of a Wisdom Explosion" (AI Impacts)** – Essay introducing and advocating the idea of a *wisdom explosion* as an alternative to a pure intelligence explosion. Leong suggests that aiming for wisdom (which includes coordination, good judgment) will mitigate many risks. He notes that a wisdom explosion can involve human-AI systems and that irresponsible actors won’t easily trigger it. This piece was directly cited in Section 6 for its definition of wisdom explosion and arguments about safety. It’s a novel contribution trying to reframe the long-term AI trajectory in positive-sum terms. It influenced our optimistic outlook that *improving AI judgment and cooperation might outpace raw power growth* if done right.

* **Hanzi Freinacht (2017), *The Listening Society* & *Nordic Ideology* (summarized in Wikipedia)** – Freinacht’s books are not online, but summaries indicate his stance on developmental patterns and metamodern politics. The Wikipedia entry captures a key quote: metamodern sociology *“always looks for meaningful explanatory developmental sequences”* and *“accepts stage theories”* where each stage is more complex. It also mentions Freinacht’s policy emphases (environmentalism, social programs). We used this to assert his moral realism: the idea that society *does* progress through stages that are non-arbitrary. It backs Section 2’s points on developmental prerequisites and Section 5’s on metamodern values guiding AI. Freinacht’s work is the backbone of our arguments about objective value-memes and the expectation of convergence at higher stages.

* **Brendan Graham Dempsey (2023), *Metamodernism: Or, The Cultural Logic of Cultural Logics* (via Wikipedia)** – Dempsey articulates metamodernism as *“recursive reflection applied to postmodernism”* yielding a *“higher vantage”* and \*“including all prior logics”\*. We referenced this to emphasize how metamodern development works (transcend and include), which parallels how we want AI to integrate prior perspectives. It’s a theoretical foundation that justifies why later perspectives can be considered “higher” (they contain previous info at a higher complexity). This supported our view that an AI reaching metamodern stage would naturally align with inclusive values.

* **Center on Long-Term Risk (2020s), writings by Lukas Gloor (e.g., "Against Irreducible Normativity")** – Gloor (and CLR folks) examine how different metaethical views alter strategy. Oesterheld cites Gloor about how certain versions of realism have strong vs weak implications. The notion that MIRI researchers “tend to reject moral realism” but philosophers mostly accept it came via Oesterheld referencing Gloor. This helps contextualize the alignment field’s bias (which we discussed in State of the Field) – showing the divide between AI researchers and philosophers.

* **OpenAI (2022), "Anthropic’s Constitutional AI" (Constitutional AI paper by Bai et al.)** – This is not directly cited above (since the user asked to ignore image references and we focused on text), but it’s important background. The method of using a fixed set of principles and having the AI self-criticize shaped our suggestions in Section 6 (like letting AI update its “constitution”). We implicitly rely on this work as proof of concept that AI can modify its outputs based on ethical rules. It’s a stepping stone to AI self-cultivation (though currently the constitution is static and human-written). We mention it to note that our ideas aren’t pure fantasy; there are nascent implementations (Anthropic’s approach) that we extrapolate from.

Each of these sources added a piece to the puzzle: Oesterheld and Armstrong gave metaethical reconciliation, Carlsmith gave the ethical stance toward AI, Freinacht and Singer gave the belief in objective values, Campolo and Constitutional AI gave concrete technical ideas, and Leong provided a forward-looking strategy concept. By synthesizing them, we built an alignment approach that is philosophically informed and technically inspired.

## **9\. Research Agenda: Toward AI Moral Self-Cultivation**

To conclude, we outline a research agenda that identifies gaps and proposes next steps to move towards AI systems capable of moral self-cultivation and aligned wisdom. The agenda is inter-disciplinary, requiring input from machine learning, philosophy, cognitive science, and more:

**9.1. Formalizing "Wisdom" in AI Systems**  
 *Open questions:* How can we represent and measure wisdom computationally? What sub-components (e.g., theory-of-mind, long-term planning, value reflection) make up wise decision-making?  
 *Next steps:*

* Develop a **“Wisdom Score”** for AI behavior, perhaps combining metrics of consistency, empathy (measured by how well AI predicts others' preferences), and foresight (did the AI avoid short-sighted decisions?). This could start as a weighted sum of proxy evaluations and be refined through expert review (e.g., ethicists rate AI decisions, and we train a model to predict those ratings as the Wisdom Score).  
* Study human **wisdom development** (via longitudinal psychological studies) and mirror those processes in AI training. For example, if meditation or reflective practices increase human compassion, can an analog (like periodic self-attention or self-critique phases) be implemented in neural networks and does it yield more prosocial behavior?  
* Pilot **multi-agent simulations** where agents are given different levels of “wisdom components” (one has empathy module on, another off, etc.) to see which configurations lead to the best group outcomes. This helps identify which components are most crucial.

**9.2. Moral Scenario Training and Evaluation**  
 *Open questions:* Can we create training environments that effectively teach AI nuanced ethical reasoning? How to avoid the AI overfitting to specific scenarios or just learning what humans *say* is right versus truly internalizing principles?  
 *Next steps:*

* Build a **library of ethical dilemmas and thought experiments** (akin to what philosophy students study) and use it to fine-tune language models. Evaluate if the fine-tuned models give more principled and consistent answers than baseline. This can test the feasibility of “philosophical education” for AI.  
* Create **interactive role-play environments** (text-based or reinforcement learning) where AI takes on roles (e.g., judge, caretaker, negotiator) that require balancing interests. After each episode, have a module or human provide feedback on the ethical quality of the AI’s choices. Research how to encode that feedback into the model’s parameter updates in a way that generalizes (e.g., using techniques from imitation learning or preference learning).  
* **Adversarial ethics testing:** similar to adversarial examples in vision but for ethics – purposely feed the AI tricky cases or cases that were outside its training distribution (e.g., a novel moral dilemma from sci-fi). See if it can reason analogically from what it knows. If it fails, that identifies a gap to address (maybe need to teach a new principle).

**9.3. Uncertainty and Self-Correction Mechanisms**  
 *Open questions:* How can an AI know when it faces a moral question it’s not equipped to answer? Can it be made to defer or seek guidance appropriately?  
 *Next steps:*

* Implement a **moral uncertainty estimator** in the AI: perhaps have the AI maintain probability distributions over a set of known moral theories (utilitarianism, deontology, etc.) as per moral uncertainty frameworks in EA. As it encounters situations, see how confident each theory is and if there’s divergence, treat it as high moral uncertainty. Train the AI that in high-uncertainty conditions, the correct action is to get clarification from humans or at least not act rashly.  
* Research **meta-learning algorithms** where the AI not only learns object-level policies but also *learns when to trust its judgment*. E.g., use a meta-reinforcement learning approach: in simulation, sometimes acting on low-confidence moral judgments leads to bad outcomes (simulated human feedback like “you should have asked me\!”). The AI meta-learns a heuristic of when to ask.  
* Carry out user studies: if the AI asks for help at certain junctures, how do human supervisors respond? Does this improve outcomes and human-AI trust? This blends technical and human factors research.

**9.4. Interdisciplinary Collaboration Pilots**  
 *Open questions:* What insights can developmental psychology provide to alignment that computer scientists might overlook? Conversely, can AI simulations validate or inform theories of moral development in humans?  
 *Next steps:*

* Host an **“AI Moral Development” workshop** with psychologists (especially those in developmental and moral psych), AI researchers, and philosophers. Set concrete goals, like designing a toy model of stage-development in an AI context or creating a cross-discipline research proposal (e.g., using an AI to model Piagetian stages).  
* Initiate a pilot study where an AI is used to simulate **Kohlberg’s moral dilemmas** and see if we can classify the AI’s answers into Kohlberg’s stages. If we train an AI on stage 3 reasoning examples vs stage 5 examples, does it generalize in stage-consistent ways? This would check if stage-like behavior can be induced and identified in AI – bridging psych theory and ML.  
* Use NLP techniques to analyze large-scale text (like social media or historical texts) for evidence of **cultural value drift** (are we seeing more “postconventional” arguments over time?). This could inform alignment by telling us what direction human values are moving (if any). It’s indirectly related but helpful for co-evolving AI with future human values.

**9.5. AI-Human Mutual Alignment**  
 *Open questions:* Alignment is usually one-way (AI aligning to human values). How do we handle cases where humans might be wrong or conflicted? Can AI provide feedback on our values without causing refusal of alignment?  
 *Next steps:*

* Explore **AI as moral advisor** systems: train AIs specifically to give ethical advice or point out human inconsistencies in a polite way. For example, fine-tune a model on texts where someone changes their mind after a logical argument. Then test if the AI can gently point out contradictions (“You value X, but your stance on Y might conflict with that because…”). Monitor human reactions in user studies – do they get defensive or find it helpful? This helps gauge if AI could have a role in *improving human alignment* with *our own* values (closing the intention-behavior gap).  
* Investigate **cooperative governance**: set up a small community simulation (could be a game or online forum) where humans and AI delegates together have to allocate resources or write a constitution. Observe how the AI participates – does it mediate, does it dominate, does it follow majority? Try different AI personas (one very egalitarian, one utilitarian etc.) and see which leads to the best group outcomes (e.g., everyone feels heard and the solution is fair). This can inform how real-world AI assistants might help committees or governments make decisions aligning with common good, without taking control.

**9.6. Safety and Monitoring of Moral RSI**  
 *Open questions:* How to ensure that an AI’s self-modification aimed at moral improvement doesn’t go awry (e.g., altering its goals unpredictably)? What theoretical guarantees or testing regimes are needed?  
 *Next steps:*

* Develop formalisms for **utility preservation under self-modification** but specifically for ethical utilities. E.g., if the AI has a reward function that includes moral terms (like a rule against lying), how to make sure a self-modification doesn’t drop those terms? This might extend existing work on stable self-improvement by adding constraints for preserving certain values.  
* Design a **“society-in-the-loop” testing protocol**: whenever the AI proposes to change its own code or values, that proposal is exposed in a sandbox to a diverse panel of simulated agents (or actual humans in a trial run) to see what would happen. Only if it passes (no catastrophic outcomes in sim, and the panel doesn’t find it objectionable) is it applied. This is like how we do software updates in critical systems (staged rollout), but with an ethical review twist. It’s partly a policy/governance research problem – how to involve oversight in an AI that is rewriting itself.  
* Research interpretability for moral reasoning: can we identify neurons or circuits in the AI that correspond to moral concepts (like a “harm detector” neuron, etc.)? If yes, we can monitor those during self-modification to ensure they are not degraded. This mixes interpretability with safety – an exciting technical challenge.

**9.7. Scaling Human Values – Coherent Extrapolated Volition 2.0**  
 *Open questions:* Yudkowsky’s CEV was never fully operationalized. Can we revisit it with modern AI? How would we practically get an AI to extrapolate our ideal values?  
 *Next steps:*

* Attempt a limited **CEV experiment**: choose a narrow domain (say, diet ethics). Have the AI model a human’s volition if they knew more and were more rational about diet (the human might currently want junk food, but ideally they want health). See if the AI can generate a recommendation that the human actually accepts as an improvement. This is a toy CEV: we know in this domain what “better informed and more rational” might imply (e.g., knowing health consequences). If it works for micro cases, gradually expand complexity.  
* Work on algorithms for **preference extrapolation**: given inconsistent preferences, is there a way to algorithmically produce a more coherent set that respects the person’s likely reflection? Economists and philosophers have proposals (e.g., Harsanyi’s preference aggregation or Rawls’ veil of ignorance) – implement these in AI and test on simulated agents with known “true” utility to see if extrapolation recovers it.  
* Address the *multiplicity of humans*: If we did CEV for all humankind, how to handle conflicts? Investigate methods like **voting within the extrapolation** (the AI could simulate a giant debate among idealized humans and see what the agreement is). This could build on deliberative democracy models. Although full CEV is far-off, these pieces are worth studying to inform long-run alignment if we ever trust an AI to do something like that.

**9.8. Ethical Governance and Policy**  
 *Open questions:* How should insights from moral realism and AI development inform AI governance? What policies ensure AIs are used to promote moral progress rather than just corporate or political interests?  
 *Next steps:*

* Propose and lobby for an **“AI Ethics Impact Assessment”** requirement (like environmental impact assessment but for large AI systems). When a new powerful AI is deployed, require evaluation of its potential moral and social effects (does it reinforce biases? Could it shift norms?). Tie this with our alignment measures – e.g., an AI with a high Wisdom Score might pass more easily. This pushes industry to incorporate alignment improvements to meet regulatory standards.  
* Encourage creation of **benchmark competitions** in the style of AI safety or ethics – e.g., the “Wisdom Challenge”: an international competition where AI systems are evaluated on complex ethical decision-making tasks and cooperation scenarios. This could galvanize research akin to how ImageNet galvanized vision. Policy can support this by funding prizes or integrating results into procurement (governments buy the AI that scores highest on wisdom/cooperation metrics, not just performance).  
* Study **public opinion and trust**: perform surveys on how much people would trust an AI that claims to be morally autonomous. Do people prefer a “tool” or a “moral partner”? This sociological research will inform how to introduce aligned AI into society without public backlash or misuse. E.g., if people are wary of AI making moral decisions, we know we need to focus on transparency and education. If they’re overly trusting, we need guardrails to prevent blind delegation.

---

This research agenda, while ambitious, is aimed at ensuring that as AI capabilities grow, so too do their *moral capacities*. By addressing these questions, we move closer to AI that is not only powerful but genuinely beneficial – an intelligence enriched with *wisdom* and *compassion* to match its knowledge. Each step – from building better moral benchmarks to creating feedback loops for self-correction – reduces the gap between human values and AI behavior. The journey to aligned AI thus becomes parallel to the journey of humanity: a continuous striving towards greater understanding, empathy, and insight.

Such an approach fulfills the prompt’s vision: not imposing a static moral checklist on AI, but cultivating an AI that *learns* and *grows* into morality. This is a profound shift, but it may be key to unlocking a future where we not only survive AI, but flourish together with it.