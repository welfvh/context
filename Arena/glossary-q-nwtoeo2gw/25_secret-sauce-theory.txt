**“secret sauce” theory**

For a long while it has seemed to me that a core crux between people like Eliezer Yudkowsky vs people like Paul Christiano is that Eliezer et al think there is some "secret sauce" to general intelligence while Paul et al disagree (or at least aren't nearly as certain about it).

This post (https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment) tries to lay out some arguments for an even stronger version of this claim than the way I'd been understanding it before. If I understand the post correctly, the claim is that once we create an AGI with this "secret sauce of generality" component (either by directly programming it or by stumbling upon it through e.g. gradient descent) then this AGI will *necessarily* have qualitatively new and very powerful general reasoning capabilities, and will also *necessarily* be an agentic expected utility maximizer. Because of the qualitatively new and different general reasoning capabilities, alignment / monitoring / control strategies that were developed or tested on AIs without this secret sauce will almost certainly fail for the new AGI, and we won't be able to use non-secret-sauce AIs to help us in any really meaningful way. And because of the agentic utility maximization + (a strong version of) the orthogonality thesis + (a strong version of) the instrumental convergence thesis, you basically need to ensure that the AGI's utility function is perfect or else it will be almost guaranteed to start seeking to aggressively self-improve, take power, and get rid of humans. So we more or less need to get it right on the first try or we are almost certainly all dead.

Conversely, if you disagree with the "secret sauce" theory then it becomes much more plausible that current alignment ideas might scale to AGI. Also it becomes much more plausible that we won't have a sudden phase change (fast takeoff) from narrow AI to powerful ASI, so we'll have more time to iterate as capabilities improve and we'll also probably have near-peer AIs that might be able to help us align / monitor / control the other AIs.

This certainly isn't the only crux in these debates, but it feels like it's a really important one. I would love it if people who are more knowledgeable than me about the different positions could weigh in and confirm / deny whether this is in fact as central a crux as this post makes it sound. (Or if something like this is a central crux but I'm totally misunderstanding it.)

Eliezer Yudkowsky Paul Christiano Nate Soares Rob Bensinger Richard Ngo Rohin Shah